# üçΩÔ∏è Voice-Enabled GenAI Restaurant Assistant (MVP)

> **AI-powered receptionist for restaurants** ‚Äî built with **FastAPI + Ollama**.
> This MVP provides a text-based conversational assistant able to take reservations, answer menu questions, and simulate customer interactions.
> Voice interactions, RAG (menu info), and multi-agent orchestration will be added in later versions.

---

## üöÄ Features (Current MVP)

‚úî Conversational restaurant assistant
‚úî Takes reservations through natural language
‚úî Answers menu + general questions
‚úî Runs locally with **Ollama** (free, open-source models)
‚úî Simple API endpoint (`/api/chat`)
‚úî Reproducible and documented

> üí° Next steps (future updates): Voice, RAG, Multi-Agents, Streamlit UI

---

## üèóÔ∏è Tech Stack

| Component          | Technology                  |
| ------------------ | --------------------------- |
| Backend            | FastAPI                     |
| LLM Inference      | Ollama (default: `mistral`) |
| HTTP Requests      | httpx                       |
| Deployment (later) | Docker                      |
| UI (later)         | Streamlit                   |

---

## üìÇ Project Structure

```
genai-restaurant-assistant/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py      # LLM call wrapper for Ollama
‚îÇ   ‚îî‚îÄ‚îÄ api.py             # FastAPI REST endpoints
‚îÇ
‚îú‚îÄ‚îÄ main.py                # FastAPI entrypoint
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Installation

### ‚öôÔ∏è 1Ô∏è‚É£ Create and activate a virtual environment

#### üñ•Ô∏è Mac / Linux

```bash
python3 -m venv venv
source venv/bin/activate
```

#### ü™ü Windows (PowerShell)

```powershell
python -m venv venv
venv\Scripts\activate
```

> üí° Always activate the venv **before running or installing anything**.

---

### 1Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2Ô∏è‚É£ Install Ollama (if not installed)

[https://ollama.com/download](https://ollama.com/download)

Then pull a model (ex: Mistral)

```bash
ollama pull mistral
```

---

## ‚ñ∂Ô∏è Run the Application

Start FastAPI server:

```bash
uvicorn main:app --reload
```

The API will be available at:

```
http://localhost:8000/api/chat
```

---

## üì° Example API Call

### Using `curl`

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
        "message": "I want to book a table for 4 tonight at 8pm",
        "history": []
      }'
```

### Example Response

```json
{
  "reply": "Sure! Can I have your name for the reservation?"
}
```

---

## üìù License

This project uses open-source, academic-friendly LLMs and tools.
You are free to use, modify, and distribute under **MIT License**.
